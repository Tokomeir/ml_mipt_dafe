{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNrs7vZwSNTS"
   },
   "source": [
    "# Семинар - Self-Supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DW6a6Y8hWtT",
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.4.2-cp310-cp310-win_amd64.whl (10.6 MB)\n",
      "     ---------------------------------------- 10.6/10.6 MB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\admin\\anaconda3\\envs\\ldw\\lib\\site-packages (from scikit-learn) (1.23.3)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "     -------------------------------------- 301.8/301.8 kB 1.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\admin\\anaconda3\\envs\\ldw\\lib\\site-packages (from scikit-learn) (1.9.2)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.4.2 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuPgrHzqkCBH"
   },
   "source": [
    "```pythin\n",
    "!pip install albumentations\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYyIZ6dphbN8"
   },
   "source": [
    "## <font color='orange'>Imports</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yQrK-eB3mBAC"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from time import gmtime, strftime\n",
    "\n",
    "import yaml\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nIqaJk_s0H3t"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BiXEOlflkCBJ"
   },
   "outputs": [],
   "source": [
    "# fix all seeds\n",
    "seed = 42\n",
    "\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMqblMrNkCBJ"
   },
   "source": [
    "# SimCLR - A Simple Framework for Contrastive Learning of Visual Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9AL45wzkCBK"
   },
   "source": [
    "За последние несколько лет мы стали свидетелями огромного прогресса в обучении с самоконтролем специально для задач, связанных с компьютерным зрением. В то время как область обработки естественного языка извлекала выгоду из достоинств обучения с самоконтролем в течение долгого времени, но это было не так давно, системы компьютерного зрения начали видеть реальное влияние парадигм обучения с самоконтролем. Такие работы, как [MoCo](https://arxiv.org/abs/1911.05722), [PIRL](https://arxiv.org/abs/1912.01991) продемонстрировали, какие преимущества системы с самоконтролем могут принести для решения проблем, связанных с компьютерным зрением.\n",
    "\n",
    "Chen at al. выпустили работу [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/pdf/2002.05709.pdf) (SimCLR), в которой представлена более простая, но эффективная структура для обучения моделей на основе компьютерного зрения с самоконтролем.\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/5ab5e0c019cdd8129b4450539231f34dc028c0cd64ba5d50db510d1ba2184160/68747470733a2f2f312e62702e626c6f6773706f742e636f6d2f2d2d764834504b704539596f2f586f3461324259657276492f414141414141414146704d2f766146447750584f79416f6b4143385868383532447a4f67457332324e68625877434c63424741735948512f73313630302f696d616765342e676966\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Оффициальный [репозиторий](https://github.com/google-research/simclr)\n",
    "\n",
    "[Статья](https://arxiv.org/abs/2011.02803)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiRJiny4kCBK"
   },
   "source": [
    "## Идея самообучения (Self-Supervision) в компьютерном зрении\n",
    "\n",
    "Для обучения моделей Self-Supervision с полностью неразмеченными данными необходимо сначала сформулировать контролируемую задачу обучения (также известную как предварительное текстовое задание) с этими неразмеченными данными. Например, для обучения встраиванию слов в большой корпус, такой как Википедия, вы можете решить задачу предсказания следующего слова по заданной последовательности слов. Итак, как вы могли бы расширить эту идею, когда дело доходит до работы с немаркированными изображениями?\n",
    "\n",
    "Важно отметить, что задача контрастного прогнозирования не должна быть ни слишком простой, ни сложной, и она должна помочь модели развить понимание заданных данных (подумайте о том, как встраивания фиксируют семантические отношения между связанными словами).\n",
    "\n",
    "Вы можете узнать больше об этом из следующей статьи [Self-supervised learning and computer vision](https://www.fast.ai/2020/01/13/self_supervised/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJqFrWR5kCBL",
    "tags": []
   },
   "source": [
    "# Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDzjBZFrkCBL"
   },
   "source": [
    "## Загрузка данных CIFAR10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HfkbyilNkCBL"
   },
   "outputs": [],
   "source": [
    "from cifar import load_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ajnAs5AgkCBM",
    "outputId": "a200a948-4207-4ae2-8dc4-21e727432f5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 32, 32, 3) (40000,)\n",
      "(10000, 32, 32, 3) (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10(\"cifar_data\", channels_last=True)\n",
    "\n",
    "class_names = np.array(['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                        'dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKhsNPkwkCBM",
    "tags": []
   },
   "source": [
    "## Аугментации с библиотекой ```albumentations``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6b1OxNRkCBM"
   },
   "source": [
    "Albumentations — это быстрая и гибкая библиотека для увеличения изображений. Библиотека широко используется в промышленности, исследованиях глубокого обучения, конкурсах по машинному обучению и проектах с открытым исходным кодом. Альбументации написаны на Python и распространяются под лицензией MIT. Исходный код доступен по адресу https://github.com/albumentations-team/albumentations.\n",
    "\n",
    "Гайды доступны на странице [Introduction to image augmentation](https://albumentations.ai/docs/#introduction-to-image-augmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qHRKf4qbkCBN"
   },
   "outputs": [],
   "source": [
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "WL8vu9DPkCBN"
   },
   "outputs": [],
   "source": [
    "# выберем случайное изображение\n",
    "idx = np.random.choice(tuple(range(len(X_train))), 1)[0]\n",
    "\n",
    "sample_image = (X_train[idx] * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VLoLxJGskCBO"
   },
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "        A.RandomRotate90(),\n",
    "        A.Flip(),\n",
    "        A.Transpose(),\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(),\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.MotionBlur(p=.2),\n",
    "            A.MedianBlur(blur_limit=3, p=0.1),\n",
    "            A.Blur(blur_limit=3, p=0.1),\n",
    "        ], p=0.2),\n",
    "        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n",
    "        A.OneOf([\n",
    "            A.OpticalDistortion(p=0.3),\n",
    "            A.GridDistortion(p=.1),\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.CLAHE(clip_limit=2),\n",
    "            A.RandomBrightnessContrast(),            \n",
    "        ], p=0.3),\n",
    "        A.HueSaturationValue(p=0.3),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "n0vnhilIkCBO",
    "outputId": "42b0734c-f3e2-4af2-cb36-21a1b08e158e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAGVCAYAAADZmQcFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAARZElEQVR4nO3cy5Nc510G4O90n+6enptmNPLMyFdFsSU7jnE5RSrFIiyAqhSwYQc7/kf+BHZUhYuzIJAYOZj47liKRhrNrfuwSBVsoNSv8xt7bD/P+tWn07fzztm83TAMQwOA39Poq74AAL4ZFAoAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJRQKACX6VYMv3t2LDl48WUT5vYO1KH9ykXVh34+j/Gw+yfKT7Hom/cpv/e/On2bXP16bZvnVvwqttda6LhtYWJtm7893nr0R5U+eHEf5n779fpT/8IOHUb611h48eBLlj8/Oo/zf/PWPo/xf/uRHUf6f/vVelL8fvt6/+JOfRPm/+vO/jfJDl/0GhpZ9p7vWRfmvu9Ho6b9hTygAlFAoAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJRYecDp7Dzb5rp2LduGWobbUONxlu+zaa42Hoc7PaMsP5tl21xr4TbXMEp3ibLP93SR5d+79zjK/9f7n0f5Tz49ivKf3T+N8tmn9TvdJNxfC9/T3/42e09/8yDbIzs9OYvyezvZHt9rd74X5btxdv63a2nravCEAkAJhQJACYUCQAmFAkAJhQJACYUCQAmFAkAJhQJACYUCQAmFAkAJhQJAiZUHtzY3sl2iySwbzzq7CLenwu2vLtzaSrfC0vNH42zrbGMte/+fhNtry3D4aDpk13N6ehHl7z9cRvnjB9n5s/D9v1hk57fW2jBkn8Ek3Hf7l39+N8rvH+5F+Wl4Pa/ffSXKHx58J8qnhiH7DuV/X2f3iK+7rnv6TcITCgAlFAoAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJVYeNFoPt6TSmZu+y7qt77Mtpuk4u/6+m0b5cZ+NYe1sZNd/sDuP8kfH2Y7RoyfZ7tTZRZYfdeEWXLiD9frtm1H+NJx5uvfeh9k/aK2dnGavoZ9l34nHJ2dR/t6vPo7yP3zr5Sj/2p03ovzm5o0on+rCe8oX+B8u+fyvH08oAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJRQKACUWHk8qB9lW0zXt2dR/vFJNv51ugy3ucLtr1mfnb82y7r59o3NKH99Zz3KP5hlO1Kf96dZ/igbw1oO2fszWmQ7VXdv70f5dz96GOVHff63Vz+ZRPlhyH4Do1G2JXV+nr2nL93M9tG+++L3ovx4lO3lxQOBtra+dJ5QACihUAAooVAAKKFQACihUAAooVAAKKFQACihUAAooVAAKKFQACihUAAosfLA1e5mtoV1uJvt9FyEW09Hp9muz+OzbNdnHO4kbYS7TTe2sm2unY15lN9az67/+RvbUf4f//2zKP/MXnb+9HAryv/snQ+i/P2H2XbZ2nr2/rfW2sXZcZRfLC+i/KjLPuNndrLP4JWXvxPl9/ZeiPItvH7bXFefJxQASigUAEooFABKKBQASigUAEooFABKKBQASigUAEooFABKKBQASigUAEqsPNC1tp5tc802su2v69NZlD8cZfn7j86j/CdHyyh/ksXbz379IMrfOsjOv/PCXpTf2My2qoZfPIjyF+EM06OHJ1H+5HQR5ZfZ9FqbLLPvc2utTWfjKH9xke3TzdfXovyrd25H+Tt3vh/lJ9NrUb7LXu4XmPLK/oMhvJ54iix2yW9QfPzTz/eEAkAJhQJACYUCQAmFAkAJhQJACYUCQAmFAkAJhQJACYUCQAmFAkAJhQJAiZUHikbjbPzofJHtGC27rNu6SbZbc3CQbVXNt7Pr/+Dz0yh/FL4/P//oUZT//Dgb6vnBH9yK8men2Tbak+PjKL+xvRXl2+hJFD85z67//OQiyrfW2tlF9m/Gk2wv79bt56L8H76VbXPtbL8Q5bvwNzy0bACvu2J//1729ld6fuuyPbt0zKtrT++Aq/UJAfC1pVAAKKFQACihUAAooVAAKKFQACihUAAooVAAKKFQACihUAAooVAAKLHyltewzHZfzsJZmWxZqbU+3KGZhltkw5DtDA1ddj19OOwzm2TXf9plu1D/+XG2tfWjP3o9yvdbG1H+p2//Msqnn9fFefZ5PT56HOVba23Wr/zzaq219uKtm1H+hz+6E+VvvXwrys839qJ8F/4GhvA33MLPON2qai27aQ1hfrE8i/JtmW3BDUOYD69/Onn6dpwnFABKKBQASigUAEooFABKKBQASigUAEooFABKKBQASigUAEooFABKKBQASqw8NnQ2pDs92e7OpMu6bTbJdpIenWQ7N0fH2brYdDyO8i18f2bZlFfbWs/en2WXXc8y3CLbnK9F+flslp2/kZ1/cpx9H3781ptRvrXWnn9+N8pfjLLP4KVbh1F+vr4d5Y9PHkT505PfRPnxKNy/C7fRuvD8dPpruQy3v8KtrVG4n9iyn2Rbhvf0ZssLgC+LQgGghEIBoIRCAaCEQgGghEIBoIRCAaCEQgGghEIBoIRCAaCEQgGgxOrjOEM2FDMssq46W2TnjxfZ7tFJNs3V8q2t7PVOwi2y2VqWH/fZ9azNplF+Em6X3XrpIMrfOMh2sNYm2fbXwV52PW++8XqUb621n7/zdpR/771PovxOn+2Xffrrd6P8k4fZ9WyvZ9+hzY1sW2y+vh7l+/D9GY2ywbwu3cJq2fszjLJ74ih8PBgN9c8TnlAAKKFQACihUAAooVAAKKFQACihUAAooVAAKKFQACihUAAooVAAKKFQACix8kBUOCvTFl3WVScXiyjfnWTn93220zOfZ+ePu2xrK76eabZV1Y+z65/22TbXJNwWe/ZgP8pPwz91+nH2fh7uvxjlP/r0syjfWmuffvowym+sz6P8eJa95vPTx9n5y+z86Tjb5ur78PWOs22ucZed34X7dKMu3P7qst/w0LJ7YgvzXbgttgpPKACUUCgAlFAoAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJRQKACUUCgAlFh5kKnrsp2bxTBE+W4UbnONsy2p9bVsR2fWllF+aNmuTz/Jrn9tlr3/s1l2/nye7STt37gR5Z/ZvxnlwymydvToOMq//8kHUf7hw2wHq7XW9vf3ovzhM1l+93r2GexuZltbm+vXovx8bSfK99ONKD/us+9oOD/Y2ij7jbWW5bvw7/ehS7e50lec3SNW4QkFgBIKBYASCgWAEgoFgBIKBYASCgWAEgoFgBIKBYASCgWAEgoFgBIKBYASK4+5TLMprDYOx5gmfbhttZZtZ21vTKP8NNzOGo2y6+/67PrXNzaj/PPP3YryO7vZztP2ta0ov7WVXf9yyL5wH316P8q30UUUv3Xr2ez81tq1jWyrav96tuU1n+9G+X6SfcaTPvvMJv16lB/CLayhy35jy3RPcAj/vg7PT3Vddo8Y4ueD7PpXmVv0hAJACYUCQAmFAkAJhQJACYUCQAmFAkAJhQJACYUCQAmFAkAJhQJACYUCQImVx3GGcJurG2c7NMtwp2dtlp1/Y2ce5dc3s62q0WQtyu/s7Uf5Z545jPLb29ei/GJYRvll66L80ePzKJ9ubd2981qUv77zXJQfd+GYXWttWGbf0XTPLt22GrpwO2v128MXyi+X4dbWKPuOLpZZvhsW2fmL8DeTXk+X/ca68PlgaNn1bG0+/R7qCQWAEgoFgBIKBYASCgWAEgoFgBIKBYASCgWAEgoFgBIKBYASCgWAEgoFgBKrb3kN2U5PH+4MtZbtDI3Xsm2u+dZulL+2k21nTTY3ovzGVrb91abZ+/n50aMof3x6EuU3N29E+ecOXojyW1vZFtnm+kGU70c7UT74qXyJsq2nbDmrtS7cekq3p4bwgsLj4/Nz2X8wXP4FfeU8oQBQQqEAUEKhAFBCoQBQQqEAUEKhAFBCoQBQQqEAUEKhAFBCoQBQQqEAUGLlgaLZLNsymq5l21w7u1tRfu/gZpTvt65H+W5zM8qfLi6i/KPP7kf5bpJtcx3ceD7K3/3ua1F+9/p+lJ9Osi21vsve/1E3i/JDuIOV7lS11loXr2dl+e6S/x4cLvn8L/CWRkbp+fEFXfILiOVrbdU8oQBQQqEAUEKhAFBCoQBQQqEAUEKhAFBCoQBQQqEAUEKhAFBCoQBQQqEAUEKhAFBi5cXH+UY23rh9LRv3e3b/RpSf91kX/vboYZQ/OT6N8vP1tSi/s5e93u/efiXKv/DcnSg/HYfjjeOdKB981VprX2DmLhz2y4cbc+l44+VOSeavuYvHA9P8Zb/iy5ZdzxBe/mWPZ17GmKQnFABKKBQASigUAEooFABKKBQASigUAEooFABKKBQASigUAEooFABKKBQASqw8sDSdTKKD57Ms//g0285a9NnQzc61a1H++cMXovwrr7wW5bev7Uf59bVs+2tjehjlh/hviyyfb21dtsv/H1Lfvtd82VthV8vlb3N99TyhAFBCoQBQQqEAUEKhAFBCoQBQQqEAUEKhAFBCoQBQQqEAUEKhAFBCoQBQYuUtr4vz8+jgbjSO8js7u1H+9q1sa+vVV96I8jcP70T5ebi1NWobUT7d2loO4XbWKNzmitLAt4EnFABKKBQASigUAEooFABKKBQASigUAEooFABKKBQASigUAEooFABKKBQASqy85fXszcPo4FdffTnKv/n6m1H+pWdfjfJbW9n213i8GeUve90qPz3c8orPBxLLNkT57mv4G/aEAkAJhQJACYUCQAmFAkAJhQJACYUCQAmFAkAJhQJACYUCQAmFAkAJhQJAiZW3vP70j/8sOvi1uz+I8td3X4zys+l2lO9G4yx/JZZxfh9f9+uHq205ZNtc58MiyvfhPWt8BX7znlAAKKFQACihUAAooVAAKKFQACihUAAooVAAKKFQACihUAAooVAAKKFQACjRDcNqgzRHRx9FB6/Pr0f50TjbrUm3qrp45kbXAv+/dMvrLNzymoZbXiNbXgB8UygUAEooFABKKBQASigUAEooFABKKBQASigUAEooFABKKBQASigUAEqsvOXVWrZbs1hkuzWjUbpDk3bhZW9/Ad8mQ3hPTLe/Rl12j7sKtyxPKACUUCgAlFAoAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJRQKACUUCgAlFh5yyvdrUnjnfEsgP+R3nO7eM0rvEmvcL4nFABKKBQASigUAEooFABKKBQASigUAEooFABKKBQASigUAEooFABKKBQASvSrBtOdmHj7C4D/NYTbXF12z10sL6L8eDR5asYTCgAlFAoAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJVbe8gLgy9OFe4jxfuIyi6/y+OEJBYASCgWAEgoFgBIKBYASCgWAEgoFgBIKBYASCgWAEgoFgBIKBYASCgWAEra8AK6gdJvr4uwsyt/7xS+j/N3vv/HUjCcUAEooFABKKBQASigUAEooFABKKBQASigUAEooFABKKBQASigUAEooFABK2PICuIIWy4so//HH/xHl//4f/i7K2/IC4EujUAAooVAAKKFQACihUAAooVAAKKFQACihUAAooVAAKKFQACihUAAocWW2vIZh+KovAeDqCO+JT84eRvl7H9+L8qvwhAJACYUCQAmFAkAJhQJACYUCQAmFAkAJhQJACYUCQAmFAkAJhQJACYUCQIlL2/Lquu6yjgb4xuv7WZRfLLPz/+2dX2X/YAWeUAAooVAAKKFQACihUAAooVAAKKFQACihUAAooVAAKKFQACihUAAooVAAKLHyltcwDNHBy2U2LGP7C/gmS++JfZ/dEx8+ehDlP/nwsyi/Ck8oAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJRQKACUUCgAlFAoAJRQKACU6IZ0pAsA/g+eUAAooVAAKKFQACihUAAooVAAKKFQACihUAAooVAAKKFQACjx35IyfyGzsVexAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "augmented_image = transform(image=sample_image)['image']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkqehdBGkCBO"
   },
   "source": [
    "### Dataset & Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ei0tT3v3kCBP"
   },
   "outputs": [],
   "source": [
    "class CLDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data, transform_augment=None):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "        assert transform_augment is not None, 'set transform_augment'\n",
    "        # TODO: pass your code\n",
    "        self.transform_augment = transform_augment\n",
    "                    \n",
    "    def __len__(self):\n",
    "        # TODO: pass your code\n",
    "        return len(self.x_data) \n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        image = self.x_data[item]\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "        label = self.y_data[item]\n",
    "        \n",
    "        # TODO: pass your code\n",
    "        x1 = self.transform_augment(image=image)['image']\n",
    "        x2 = self.transform_augment(image=image)['image']\n",
    "        \n",
    "        image = torch.tensor(image).permute(2, 0, 1)\n",
    "        \n",
    "        return x1, x2, label, image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12Ya9DHvkCBP"
   },
   "source": [
    "### Задание № 2\n",
    "\n",
    "Расчитайте среднее значение и дисперсию для функции нормализации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VByEPX20kCBP"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# TODO: pass your code\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m MEAN \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mX_train\u001b[49m, axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m      3\u001b[0m STD \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(X_train, axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: pass your code\n",
    "MEAN = np.mean(X_train, axis=(0, 1, 2), keepdims=True).squeeze()\n",
    "STD = np.std(X_train, axis=(0, 1, 2), keepdims=True).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5lCZZ00kCBP"
   },
   "source": [
    "### Задание № 3\n",
    "\n",
    "Задайте преобразования для шага обучения и шага валидации. На валидации не должно быть преобразований, изменяющих исходные данные. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "0VesV1eNkCBP"
   },
   "outputs": [],
   "source": [
    "# TODO: pass your code\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.OneOf([\n",
    "        A.ColorJitter(),\n",
    "        A.ToGray(),\n",
    "#         A.GaussNoise(),\n",
    "    ]),\n",
    "#     A.OneOf([\n",
    "#         A.Cutout(num_holes=1, max_h_size=10, max_w_size=10),\n",
    "#         A.RandomResizedCrop(32, 32),\n",
    "#         A.GaussianBlur(),\n",
    "#     ]),\n",
    "    A.HorizontalFlip(),\n",
    "#     A.RandomRotate90(),\n",
    "    A.Normalize(mean=MEAN, std=STD),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "    A.Normalize(mean=MEAN, std=STD),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1KgVjfnUkCBP"
   },
   "outputs": [],
   "source": [
    "train_dataset = CLDataset(X_train, y_train, transform_augment=train_transform)\n",
    "valid_dataset = CLDataset(X_val, y_val, transform_augment=valid_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "fKf29XVSkCBP"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_workers = 0\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=batch_size,       \n",
    "                                           shuffle=True, \n",
    "                                           num_workers=n_workers)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZvhKKJjkCBQ"
   },
   "source": [
    "### Проверка итераций загрузчика данных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "SvqJ1_3WkCBQ"
   },
   "outputs": [],
   "source": [
    "def get_cropped_data_idxs(data, crop_coef: float = 1.0):\n",
    "    crop_coef = np.clip(crop_coef, 0, 1)\n",
    "    \n",
    "    init_data_size = len(data)\n",
    "    final_data_size = int(init_data_size * crop_coef)\n",
    "    \n",
    "    random_idxs = np.random.choice(tuple(range(init_data_size)), final_data_size, replace=False)\n",
    "    return random_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "2e-GlfAqkCBQ"
   },
   "outputs": [],
   "source": [
    "def load_datasets(X_train, y_train, X_val, y_val, train_transform, valid_transform, crop_coef=0.2):\n",
    "    train_idxs = get_cropped_data_idxs(X_train, crop_coef=crop_coef)\n",
    "    train_data = X_train[train_idxs]\n",
    "    train_labels = y_train[train_idxs] \n",
    "\n",
    "    valid_idxs = get_cropped_data_idxs(X_val, crop_coef=crop_coef)\n",
    "    valid_data = X_val[valid_idxs]\n",
    "    valid_labels = y_val[valid_idxs] \n",
    "    \n",
    "    train_dataset = CLDataset(train_data, train_labels, transform_augment=train_transform)\n",
    "    valid_dataset = CLDataset(valid_data, valid_labels, transform_augment=valid_transform)\n",
    "    \n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ZZzV2s7zkCBR",
    "outputId": "8a079068-9d88-407e-cc07-d37485507983"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 40000 Valid size: 10000\n"
     ]
    }
   ],
   "source": [
    "train_dataset, valid_dataset = load_datasets(X_train, y_train, X_val, y_val, crop_coef=1.4)\n",
    "print('Train size:', len(train_dataset), 'Valid size:', len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0,\n",
    "                          pin_memory=True,\n",
    "                          drop_last=True\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "9OaJLF2QkCBR"
   },
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 out_features,\n",
    "                 use_bias = True,\n",
    "                 use_bn = False,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = use_bias\n",
    "        self.use_bn = use_bn\n",
    "        \n",
    "        self.linear = nn.Linear(self.in_features, \n",
    "                                self.out_features, \n",
    "                                bias = self.use_bias and not self.use_bn)\n",
    "        if self.use_bn:\n",
    "             self.bn = nn.BatchNorm1d(self.out_features)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.linear(x)\n",
    "        if self.use_bn:\n",
    "            x = self.bn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "PYkMcwWYmawC"
   },
   "outputs": [],
   "source": [
    "def l2_norm(input, axis=1):\n",
    "    norm = torch.norm(input, 2, axis, True)\n",
    "    output = torch.div(input, norm)\n",
    "    return output\n",
    "\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 hidden_features,\n",
    "                 out_features,\n",
    "                 head_type = 'nonlinear',\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.head_type = head_type\n",
    "\n",
    "        if self.head_type == 'linear':\n",
    "            self.layers = LinearLayer(self.in_features, self.out_features, use_bias=False, use_bn=True)\n",
    "        elif self.head_type == 'nonlinear':\n",
    "            self.layers = nn.Sequential(\n",
    "                LinearLayer(self.in_features, self.hidden_features, use_bias=True, use_bn=True),\n",
    "                nn.ReLU(),\n",
    "                LinearLayer(self.hidden_features, self.out_features, use_bias=False, use_bn=True))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = l2_norm(x)\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class PreModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # pretrained model        \n",
    "        model = torchvision.models.resnet50(pretrained=True)\n",
    "        self.encoder = nn.Sequential(*tuple(model.children())[:-1])\n",
    "        \n",
    "        emb_size = tuple(model.children())[-1].in_features\n",
    "        \n",
    "        for p in self.encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        self.projector = ProjectionHead(emb_size, 2048, 128)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.encoder(x)\n",
    "        \n",
    "        xp = self.projector(torch.squeeze(out))\n",
    "        \n",
    "        return xp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "CHJYDr2kFG1J",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keleas/miniconda3/envs/general/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/keleas/miniconda3/envs/general/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = 'mps'\n",
    "model = PreModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "S4jCRPqKpl_i"
   },
   "outputs": [],
   "source": [
    "x = np.random.random((32,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "_P9kuZf1FbXk",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out = model(torch.tensor(x, device=device, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2ZXrAypFN1r",
    "outputId": "26a61aca-47c8-44ba-b669-fb07cae2c657"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEaRSe00ZRIO",
    "tags": []
   },
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zSuiZ5zpl_j",
    "tags": []
   },
   "source": [
    "## Функция ошибки\n",
    "\n",
    "Normalized Temperature-Scaled Cross-Entropy Loss (NT-XEnt loss)\n",
    "\n",
    "Теперь, когда архитектура описана, давайте подробнее рассмотрим, как мы обучаем модель. Как упоминалось ранее, мы хотим максимизировать сходство между представлениями двух дополненных версий одного и того же изображения на рисунке выше, сводя его к минимуму для всех других примеров в пакете. Короче говоря, функция ошибки сравнивает сходство $z_i$ и $z_j$\n",
    "к любому другому представлению в пакете, выполнив softmax над значениями подобия. Ошибка может быть формально записана как:\n",
    "\n",
    "$$\n",
    "\\ell_{i,j}=-\\log \\frac{\\exp(\\text{sim}(z_i,z_j)/\\tau)}{\\sum_{k=1}^{2N}\\mathbb{1}_{[k\\neq i]}\\exp(\\text{sim}(z_i,z_k)/\\tau)}=-\\text{sim}(z_i,z_j)/\\tau+\\log\\left[\\sum_{k=1}^{2N}\\mathbb{1}_{[k\\neq i]}\\exp(\\text{sim}(z_i,z_k)/\\tau)\\right]\n",
    "$$\n",
    "\n",
    "где\n",
    "$$\n",
    "\\text{sim}(z_i,z_j) = \\frac{z_i^\\top \\cdot z_j}{||z_i||\\cdot||z_j||}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "P1Ileef6pl_j"
   },
   "outputs": [],
   "source": [
    "class SimCLR_Loss(nn.Module):\n",
    "    def __init__(self, batch_size, temperature):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.mask = self.mask_correlated_samples(batch_size)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
    "        \n",
    "        self.tot_neg = 0\n",
    "\n",
    "    def mask_correlated_samples(self, batch_size):\n",
    "        N = 2 * batch_size\n",
    "        mask = torch.ones((N, N), dtype=bool)\n",
    "        mask = mask.fill_diagonal_(0)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            mask[i, batch_size + i] = 0\n",
    "            mask[batch_size + i, i] = 0\n",
    "            \n",
    "        return mask\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        N = 2 * self.batch_size\n",
    "\n",
    "        z = torch.cat((z_i, z_j), dim=0)\n",
    "\n",
    "        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
    "\n",
    "        sim_i_j = torch.diag(sim, self.batch_size)\n",
    "        sim_j_i = torch.diag(sim, -self.batch_size)\n",
    "        \n",
    "        # We have 2N samples, but with Distributed training every GPU gets N examples too, resulting in: 2xNxN\n",
    "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
    "        negative_samples = sim[self.mask].reshape(N, -1)\n",
    "        \n",
    "        # SIMCLR\n",
    "        labels = torch.from_numpy(np.array([0] * N)).reshape(-1).to(positive_samples.device).long()\n",
    "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= N\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9119, dtype=torch.float64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.tensor(np.random.random((4, 128)))\n",
    "x2 = torch.tensor(np.random.random((4, 128)))\n",
    "\n",
    "loss = SimCLR_Loss(batch_size=4, temperature=0.2).to(device)\n",
    "loss(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2XOwfizpl_j",
    "tags": []
   },
   "source": [
    "## Optimizer\n",
    "\n",
    "### LARS \n",
    "\n",
    "Обычный способ ускорить обучение больших сверточных сетей — добавить вычислительные единицы. Затем обучение выполняется с использованием параллельных синхронных данных. С увеличением количества ГПУ увеличивается размер пакета (batch size). Но обучение с большим размером партии часто приводит к снижению точности модели. Авторы публикации [LARS](https://arxiv.org/pdf/1708.03888.pdf) предлагают способ, как этого можно избежать.\n",
    "\n",
    "Реализация [LARS](https://github.com/Spijkervet/SimCLR/blob/cd85c4366d2e6ac1b0a16798b76ac0a2c8a94e58/simclr/modules/lars.py) для ```pytorch```.\n",
    "\n",
    "Однако для упрощения примера мы рассмотрим обучение через Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJPC4wYwpl_l"
   },
   "source": [
    "## Визуализация эмбедингов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ul3TtCT0kCBT"
   },
   "source": [
    "## Класс обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "WvUOo8bGkCBT"
   },
   "outputs": [],
   "source": [
    "class BaseTrainProcess:\n",
    "    def __init__(self, hyp):\n",
    "        self.best_loss = 1e100\n",
    "        self.best_acc = 0.0\n",
    "        self.current_epoch = -1\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.hyp = hyp\n",
    "\n",
    "        self.lr_scheduler: Optional[torch.optim.lr_scheduler] = None\n",
    "        self.model: Optional[torch.nn.modules] = None\n",
    "        self.optimizer: Optional[torch.optim] = None\n",
    "        self.criterion: Optional[torch.nn.modules] = None\n",
    "\n",
    "        self.train_loader: Optional[Dataloader] = None\n",
    "        self.valid_loader: Optional[Dataloader] = None\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def _init_data(self, X_train, y_train, X_val, y_val, train_transform, valid_transform):\n",
    "\n",
    "        train_dataset, valid_dataset = load_datasets(X_train, y_train, X_val, y_val, crop_coef=1.4)\n",
    "        print('Train size:', len(train_dataset), 'Valid size:', len(valid_dataset))\n",
    "\n",
    "        self.train_loader = DataLoader(train_dataset,\n",
    "                                       batch_size=self.hyp['batch_size'],\n",
    "                                       shuffle=True,\n",
    "                                       num_workers=self.hyp['n_workers'],\n",
    "                                       pin_memory=True,\n",
    "                                       drop_last=True\n",
    "                                      )\n",
    "\n",
    "        self.valid_loader = DataLoader(valid_dataset,\n",
    "                                       batch_size=self.hyp['batch_size'],\n",
    "                                       shuffle=True,\n",
    "                                       num_workers=self.hyp['n_workers'],\n",
    "                                       pin_memory=True,\n",
    "                                       drop_last=True\n",
    "                                      )\n",
    "\n",
    "    def _init_model(self):\n",
    "        self.model = PreModel()\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        model_params = [params for params in self.model.parameters() if params.requires_grad]\n",
    "        # self.optimizer = LARS(model_params, lr=0.2, weight_decay=1e-4)\n",
    "        self.optimizer = torch.optim.AdamW(model_params, lr=self.hyp['lr'], weight_decay=self.hyp['weight_decay'])\n",
    "\n",
    "        # \"decay the learning rate with the cosine decay schedule without restarts\"\n",
    "        self.warmupscheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lambda epoch: (epoch + 1) / 10.0)\n",
    "        self.mainscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            self.optimizer,\n",
    "            500,\n",
    "            eta_min=0.05,\n",
    "            last_epoch=-1,\n",
    "        )\n",
    "        \n",
    "        self.criterion = SimCLR_Loss(batch_size=self.hyp['batch_size'], \n",
    "                                     temperature=self.hyp['temperature']).to(self.device)\n",
    "\n",
    "    def init_params(self):\n",
    "        self._init_data()\n",
    "        self._init_model()\n",
    "\n",
    "    def save_checkpoint(self, loss_valid, path):\n",
    "        if loss_valid[0] <= self.best_loss:\n",
    "            self.best_loss = loss_valid[0]\n",
    "            self.save_model(path)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.mainscheduler.state_dict()\n",
    "        }, path)\n",
    "\n",
    "    def train_step(self):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        cum_loss = 0.0\n",
    "        proc_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(enumerate(self.train_loader), total=len(self.train_loader),\n",
    "                    desc=f'Train {self.current_epoch}/{self.hyp[\"epochs\"] - 1}')\n",
    "        for idx, (xi, xj, _, _) in pbar:\n",
    "            xi, xj = xi.to(self.device), xj.to(self.device)\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                zi = self.model(xi)\n",
    "                zj = self.model(xj)\n",
    "                loss = self.criterion(zi, zj)\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                self.model.zero_grad()\n",
    "\n",
    "            cur_loss = loss.detach().cpu().numpy()\n",
    "            cum_loss += cur_loss\n",
    "\n",
    "            proc_loss = (proc_loss * idx + cur_loss) / (idx + 1)\n",
    "\n",
    "            s = f'Train {self.current_epoch}/{self.hyp[\"epochs\"] - 1}, Loss: {proc_loss:4.3f}'\n",
    "            pbar.set_description(s)\n",
    "\n",
    "        cum_loss /= len(self.train_loader)\n",
    "        return [cum_loss]\n",
    "\n",
    "    def valid_step(self):\n",
    "        self.model.eval()\n",
    "\n",
    "        cum_loss = 0.0\n",
    "        proc_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(enumerate(self.valid_loader), total=len(self.valid_loader),\n",
    "                    desc=f'Valid {self.current_epoch}/{self.hyp[\"epochs\"] - 1}')\n",
    "        for idx, (xi, xj, _, _) in pbar:\n",
    "            xi, xj = xi.to(self.device), xj.to(self.device)\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "                zi = self.model(xi)\n",
    "                zj = self.model(xj)\n",
    "                loss = self.criterion(zi, zj)\n",
    "\n",
    "            cur_loss = loss.detach().cpu().numpy()\n",
    "            cum_loss += cur_loss\n",
    "\n",
    "            proc_loss = (proc_loss * idx + cur_loss) / (idx + 1)\n",
    "\n",
    "            s = f'Valid {self.current_epoch}/{self.hyp[\"epochs\"] - 1}, Loss: {proc_loss:4.3f}'\n",
    "            pbar.set_description(s)\n",
    "\n",
    "        cum_loss /= len(self.valid_loader)\n",
    "        return [cum_loss]\n",
    "\n",
    "    def run(self):\n",
    "        best_w_path = 'best.pt'\n",
    "        last_w_path = 'last.pt'\n",
    "        \n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "\n",
    "        for epoch in range(self.hyp['epochs']):\n",
    "            self.current_epoch = epoch\n",
    "\n",
    "            loss_train = self.train_step()\n",
    "            train_losses.append(loss_train)\n",
    "                \n",
    "            if epoch < 10:\n",
    "                self.warmupscheduler.step()\n",
    "            else:\n",
    "                self.mainscheduler.step()\n",
    "\n",
    "            lr = self.optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "            loss_valid = self.valid_step()\n",
    "            valid_losses.append(loss_valid)\n",
    "            \n",
    "            #self.save_checkpoint(loss_valid, best_w_path)\n",
    "            \n",
    "        self.save_model(last_w_path)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "2ifBxkFukCBU",
    "outputId": "ce14b49d-ebe8-42de-c093-48c03b8c70c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'lr': 0.2, 'lrf': 0.02, 'epochs': 50, 'weight_decay': 0.0004, 'n_workers': 0, 'temperature': 0.5, 'seed': 42}\n"
     ]
    }
   ],
   "source": [
    "with open('hyp_params.yaml', 'r') as f:\n",
    "    hyps = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "    \n",
    "print(hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "E2Of-FNckCBU",
    "outputId": "6717de95-a1ec-44e8-d51f-663c51c612b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log dir: logs/2024-04-06 06-17-30\n",
      "Train size: 40000 Valid size: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keleas/miniconda3/envs/general/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/keleas/miniconda3/envs/general/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 40000 Valid size: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keleas/miniconda3/envs/general/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/keleas/miniconda3/envs/general/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "set_seed(hyps['seed'])\n",
    "\n",
    "trainer = BaseTrainProcess(hyps)\n",
    "trainer.device = 'mps'\n",
    "trainer.init_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "bjW_XQolkCBU",
    "outputId": "5e241963-dc56-45e3-eae3-2cec35c5e716"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 0/49, Loss: 4.383: 100%|██████████| 312/312 [01:30<00:00,  3.46it/s]\n",
      "Valid 0/49, Loss: 3.757: 100%|██████████| 78/78 [00:18<00:00,  4.13it/s]\n",
      "Train 1/49, Loss: 4.257:  77%|███████▋  | 240/312 [01:15<00:28,  2.53it/s]"
     ]
    }
   ],
   "source": [
    "train_losses, valid_losses = trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQB7CoKTkCBU",
    "outputId": "9e0b5623-925f-4101-ae06-772d254894a3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmw-WH7EkCBV"
   },
   "source": [
    "# Литература и ссылки\n",
    "- Документация [Albumentations](https://albumentations.ai/)\n",
    "- Предобученные модели [torchvision](https://pytorch.org/vision/stable/models.html)\n",
    "- [LARGE BATCH TRAINING OF CONVOLUTIONAL NETWORKS](https://arxiv.org/pdf/1708.03888.pdf)\n",
    "- [SimCLR](https://arxiv.org/pdf/2002.05709.pdf)\n",
    "- [Self-supervised learning and computer vision](https://www.fast.ai/2020/01/13/self_supervised/)\n",
    "- [Pytorch SimCLR](https://github.com/sthalles/SimCLR)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "NwHbQkep1MLx",
    "uxVU4yN2fmNf",
    "ziLMBZNX5M38",
    "zB7FcvNU5TyH",
    "yS0S5_Qr5fKm",
    "K6kf2F-K5pw7",
    "6qoGDc-05wOG",
    "fhlUTwvc53jw",
    "1QF5TgaoUQgz",
    "C969RA6x3zKK"
   ],
   "name": "seminar_4_solved.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "625px",
    "width": "382px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
